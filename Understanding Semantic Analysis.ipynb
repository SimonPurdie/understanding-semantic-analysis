{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6beabbf0"
   },
   "source": [
    "# Understanding Semantic Analysis\n",
    "\n",
    "In this notebook, we'll look at how we can use a computer to perform *Semantic Analysis*. Looking at some text, and extracting information relating to the **meaning** of that text.\n",
    "\n",
    "We're going to try to grasp what's really happening and why it works, to help us form accurate intuitions about how to use these tools.\n",
    "\n",
    "We're going to look at a very common application of semantic analysis called *Sentiment Analysis* - trying to determine what opinions are being expressed about a thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f7e6d8"
   },
   "source": [
    "First, we need to install some software libraries.\n",
    "\n",
    "***sentence-transformers*** is a model trained to turn sentences into a form that can be understood by machines. It converts them into a **vector**, which is basically a kind of position in language space. In that language space, texts that relate to similar things are closer to each other, and texts about totally different things are further apart.\n",
    "\n",
    "***scikit-learn*** is a library that gives us the math tools to compare those positions. Language space has lots of dimensions! That's really difficult to visualise for humans, since our brains are really set up to deal with three dimensional space, but in maths it all basically works the same way whether you have 3 dimensions or 384 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4604,
     "status": "ok",
     "timestamp": 1768411790548,
     "user": {
      "displayName": "simon purdie",
      "userId": "08862187727402748240"
     },
     "user_tz": 0
    },
    "id": "8adeac64"
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e507a90c"
   },
   "source": [
    "We're going to analyse this small set of product reviews, and use some test phrases to compare against the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768413130388,
     "user": {
      "displayName": "simon purdie",
      "userId": "08862187727402748240"
     },
     "user_tz": 0
    },
    "id": "09e54575"
   },
   "outputs": [],
   "source": [
    "product_reviews = [\n",
    "    \"This product is excellent, very happy with my purchase.\",\n",
    "    \"It broke after a week.\",\n",
    "    \"Good value for money, works as expected.\",\n",
    "    \"Absolutely terrible, a complete waste of money.\",\n",
    "    \"I love this item, it's perfect for my needs.\",\n",
    "    \"It's okay, nothing special but it gets the job done.\",\n",
    "    \"Very durable and well-made, highly recommend.\"\n",
    "]\n",
    "\n",
    "test_phrases = [\n",
    "    \"the product is good quality\",\n",
    "    \"the product is poor quality\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3c4400a"
   },
   "source": [
    "Here we load our model for turning reviews into positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1768411791112,
     "user": {
      "displayName": "simon purdie",
      "userId": "08862187727402748240"
     },
     "user_tz": 0
    },
    "id": "6e48bdea",
    "outputId": "1b61655d-1ae9-426f-d537-94fc2ecb6689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformer model 'all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Sentence Transformer model 'all-MiniLM-L6-v2' loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1ce2eb1"
   },
   "source": [
    "And here we use the model to get the positions of each of our reviews in our language space.\n",
    "\n",
    "---\n",
    "\n",
    "*Our \"position\" is a* ***vector*** *(a direction and a distance) describing where it is, compared to the centre of our language space.*\n",
    "\n",
    "*A vector used for this purpose - to capture relationships and meaning - is also called an* ***embedding***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1768413137159,
     "user": {
      "displayName": "simon purdie",
      "userId": "08862187727402748240"
     },
     "user_tz": 0
    },
    "id": "3542912a",
    "outputId": "e08a4f01-b6a9-4596-9295-976bdb67ae26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 7 reviews\n",
      "Generated embeddings for 2 test phrases\n"
     ]
    }
   ],
   "source": [
    "product_review_embeddings = model.encode(product_reviews)\n",
    "test_phrase_embeddings = model.encode(test_phrases)\n",
    "\n",
    "print(f\"Generated embeddings for {len(product_reviews)} reviews\")\n",
    "print(f\"Generated embeddings for {len(test_phrases)} test phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72a6f020"
   },
   "source": [
    "Now we compare the positions of each of our reviews to the positions of our test phrases. We're looking at how similar their vectors are. This is called *Vector Similarity*.\n",
    "\n",
    "And we'll be using a particular kind of vector similiarity technique here called *Cosine Similarity*. Instead of looking at how *close* our positions are, we're looking to see if the vectors are *pointing in similar directions*.\n",
    "\n",
    "The reason for this is that two positions can be far apart but similiar in meaning. If we imagine two reviews:\n",
    "\n",
    "    \"I like the product packaging\"\n",
    "    \"I LOVE THE PRODUCT PACKAGING ITS MY FAVORITE THING IN THE WORLD\n",
    "     I LOVE IT SO MUCH I HAVE SOLD ALL MY WORLDLY POSSESSIONS\n",
    "     IN ORDER TO OBTAIN MORE OF THE PRODUCT PACKAGING\n",
    "     WHICH I LOVE\"\n",
    "\n",
    "These two reviews have similar meaning, but the second is far greater in *magnitude*. It will be *further out* in our language space, but both reviews should be in a *similar direction* from the centre of our space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a visual aid to help us understand these relationships of direction vs magnitude in our space:\n",
    "\n",
    "![A plot visualising how vector metrics interpret similarity. Euclidean distance groups \"cat\" and \"dog\" because they are short, related words. Cosine similarity aligns \"cat\" with its long definition \"felis cattus, the domestic cat\" because they share the same semantic direction, demonstrating why angular similarity is better for matching meaning.](https://raw.githubusercontent.com/SimonPurdie/understanding-semantic-analysis/refs/heads/master/assets/semantic_cats_and_dogs.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1768413141750,
     "user": {
      "displayName": "simon purdie",
      "userId": "08862187727402748240"
     },
     "user_tz": 0
    },
    "id": "29aa7dbe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_scores = cosine_similarity(product_review_embeddings, test_phrase_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ea98dc2"
   },
   "source": [
    "So now we have simple number values showing how close in meaning our reviews are to our two anchors:\n",
    "\n",
    "    the product is good quality\n",
    "    the product is poor quality\n",
    "\n",
    "And we can use the difference between those values to determine which of our anchors the review is closer to, extracting the sentiment from it.\n",
    "\n",
    "---\n",
    "\n",
    "It would also be possible to use our values to check reviews were sufficiently *relevant* to either anchor, in case our dataset included reviews like:\n",
    "\n",
    "    stop asking me questions\n",
    "    siri play agadoo by black lace\n",
    "    purple monkey dishwasher\n",
    "\n",
    "but for this notebook we'll just assume our reviews are highly relevant and comprehensible, as I'm sure most online reviews would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1768413143886,
     "user": {
      "displayName": "simon purdie",
      "userId": "08862187727402748240"
     },
     "user_tz": 0
    },
    "id": "b9608ac4",
    "outputId": "620ccec0-1014-41dd-dc01-5aa2a639fd5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentiment Analysis Results ---\n",
      "Review: This product is excellent, very happy with my purchase.\n",
      "Score: 0.1565\n",
      "Result: Positive\n",
      "\n",
      "Review: It broke after a week.\n",
      "Score: -0.0753\n",
      "Result: Negative\n",
      "\n",
      "Review: Good value for money, works as expected.\n",
      "Score: 0.0713\n",
      "Result: Positive\n",
      "\n",
      "Review: Absolutely terrible, a complete waste of money.\n",
      "Score: -0.0625\n",
      "Result: Negative\n",
      "\n",
      "Review: I love this item, it's perfect for my needs.\n",
      "Score: 0.1284\n",
      "Result: Positive\n",
      "\n",
      "Review: It's okay, nothing special but it gets the job done.\n",
      "Score: 0.0893\n",
      "Result: Positive\n",
      "\n",
      "Review: Very durable and well-made, highly recommend.\n",
      "Score: 0.2622\n",
      "Result: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = []\n",
    "\n",
    "for scores in cosine_scores:\n",
    "    good_quality_score = scores[0]\n",
    "    bad_quality_score = scores[1]\n",
    "\n",
    "    # We calculate the score by finding the difference.\n",
    "    # A positive number means it's leaning toward \"Good\", negative toward \"Bad\".\n",
    "    score_diff = good_quality_score - bad_quality_score\n",
    "    sentiment_scores.append(score_diff)\n",
    "\n",
    "print(\"--- Sentiment Analysis Results ---\")\n",
    "\n",
    "for i, review in enumerate(product_reviews):\n",
    "    current_score = sentiment_scores[i]\n",
    "\n",
    "    # If the score is greater than 0, it's on the 'Positive' side of language space\n",
    "    if current_score > 0:\n",
    "        label = \"Positive\"\n",
    "    else:\n",
    "        label = \"Negative\"\n",
    "\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Score: {current_score:.4f}\")\n",
    "    print(f\"Result: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSSC2JGkS6pl"
   },
   "source": [
    "We can see the model successfully picked up on the sentiment in each review!\n",
    "\n",
    "Of course, this method isn't perfect. Our results depend a lot on which anchor phrases we chose - different test phrases would give different scores.\n",
    "\n",
    "Some scores don't completely match my intuitions either. I thought:\n",
    "\n",
    "    This product is excellent, very happy with my purchase.\n",
    "\n",
    "sounded a bit more enthusiastic and positive than\n",
    "\n",
    "    Very durable and well-made, highly recommend.\n",
    "\n",
    "but the second scored much higher. So the way that the model parses meaning isn't necessarily the same way I would.\n",
    "\n",
    "And real language usage can get very messy and complicated. What about sarcasm or mixed feelings? Those are much trickier!\n",
    "\n",
    "So this technique has imperfections, but it can also be an extremely powerful tool for extracting meaning from large natural language datasets."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOfIjvXorO/9qidULArmYRp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
